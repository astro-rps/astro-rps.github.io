{"kind":"Notebook","sha256":"2e66ef90d1162f7c98a2af22c824aaaf3f0a2bd5cad9dad3b32aaf05fc063042","slug":"week8-linear-regression","location":"/01_Lectures/Week 8/week8_linear_regression.ipynb","dependencies":[],"frontmatter":{"title":"Introduction to Inference in Astronomy: Fitting Models to Data","github":"https://astro-rps.github.io/","keywords":[],"thumbnail":"/build/bootstrap_resampling-19edf9abbf7bf206cfe7896795c20131.png","exports":[{"format":"ipynb","filename":"week8_linear_regression.ipynb","url":"/build/week8_linear_regress-d17bd27e8bb7def29ce3a5b282865361.ipynb"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Today, we’ll explore what data science entails, the importance of model fitting and how we can apply it in astronomical data analysis, and the role of inference in drawing meaningful conclusions from our observations.","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"yXC5J9Pcnz"}],"key":"l743RW3TA3"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Inference refers to the process of comparing models to data and is a fundamental aspect of data analysis in astronomy. In the context of astronomy, model fitting enables us to extract valuable information about the physical properties of objects (like stars, galaxies, exoplanets, etc) by comparing observational data to theoretical models or mathematical functions.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"YTYPSPWavg"}],"key":"t12Sdj0ELC"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"This notebook will cover two of the most fundamental models with use in astronomy: lines and gaussian curves.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"HeJaV8LZke"}],"key":"zx3kSVMLOo"}],"data":{"type":"notebook-content"},"key":"T5HCvsdcww"},{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"Linear Regression:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"pb1a4jgVpj"}],"identifier":"linear-regression","label":"Linear Regression:","html_id":"linear-regression","implicit":true,"key":"a4HQxyNuak"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Linear regression just means “let’s fit a line”! It’s a statistical method used to model the relationship between a dependent variable (often denoted as ‘y’) and one or more independent variables (often denoted as ‘x’). It assumes that the relationship between the variables can be described by a linear equation, such as a straight line in the case of simple linear regression. Like an inference task, the goal of linear regression is to find the best-fitting line that minimizes the differences between the observed values and the values predicted by the model.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"C8CM3fHSY2"}],"key":"tusP0xQ9aw"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"For the simpliest case of a straight line, aka a first order polynomial, we can describe our model using two parameters: ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"CPvSY9nM6B"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"slope","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"NlEiCilHdI"}],"key":"BniN7x72GP"},{"type":"text","value":" and ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"rNtaJp8XIi"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"intercept","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"YyvRoNeoc6"}],"key":"vDoljvnR0Z"},{"type":"text","value":".","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"AejlkYuFBO"}],"key":"upN0lsZoFO"}],"data":{"type":"notebook-content"},"key":"ITjyaKdfn1"},{"type":"block","children":[{"type":"heading","depth":3,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"How can we implement this in python?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"DyPrGlViDe"}],"identifier":"how-can-we-implement-this-in-python","label":"How can we implement this in python?","html_id":"how-can-we-implement-this-in-python","implicit":true,"key":"QoTUZ3N3iP"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"inlineCode","value":"np.polyfit","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"d59QXRknhR"},{"type":"text","value":" is a function in NumPy used for polynomial regression, which fits a polynomial curve to a set of data points. In the context of linear regression, ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"mGfez1jZJf"},{"type":"inlineCode","value":"np.polyfit","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"SQqZHHG0DI"},{"type":"text","value":" fits a polynomial of degree 1 (a straight line) to the given data points by minimizing the sum of squared errors between the observed and predicted values. This function returns the coefficients of the polynomial that best fits the data, allowing us to determine the slope and intercept of the regression line.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"KcVDNfWiAo"}],"key":"dNoYbXTVJa"}],"data":{"type":"notebook-content"},"key":"NfdQQhvEnn"},{"type":"block","children":[{"type":"code","lang":"python","executable":true,"value":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate fake data for demonstration\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2.1, 3.9, 5.8, 8.2, 10.1])","key":"CglEvT2SzP"},{"type":"output","id":"BT0rky_pXbt2XCyrJAP39","data":[],"key":"au84GaX1mM"}],"data":{"type":"notebook-code"},"key":"hhNYv3R21r"},{"type":"block","children":[{"type":"code","lang":"python","executable":true,"value":"# Fit a straight line (polynomial of degree 1) to the data\ncoefficients = np.polyfit(x, y, deg=1)\ncoefficients","key":"s7cglXjObQ"},{"type":"output","id":"EM2lDEP2GgD0afmdms83k","data":[{"output_type":"execute_result","execution_count":6,"metadata":{},"data":{"text/plain":{"content":"array([ 2.03, -0.07])","content_type":"text/plain"}}}],"key":"l2xHBgKOkh"}],"data":{"type":"notebook-code"},"key":"nuUVmD2lFY"},{"type":"block","children":[{"type":"code","lang":"python","executable":true,"value":"# Extract the slope (m) and intercept (b) from the coefficients\nslope, intercept = coefficients\n# note: you can do this more efficiently by just writing: slope, intercept = np.polyfit(x, y, deg=1),\n# but I wanted to print coefficients to show the actual output of polyfit\n\n# Print the equation of the fitted line\nprint(f\"Fitted line equation: y = {slope:.2f}x + {intercept:.2f}\")","key":"RZIvR2n2zR"},{"type":"output","id":"qp1pdtv0qh0WAw-vv4IKq","data":[{"name":"stdout","output_type":"stream","text":"Fitted line equation: y = 2.03x + -0.07\n"}],"key":"M7G2vXROXF"}],"data":{"type":"notebook-code"},"key":"g801ml46Sm"},{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So we have managed to fit a line to our data! Horray! Now let’s plot both the data and model:","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"ZWzeK0JgSN"}],"key":"tOsvxyM0tY"}],"data":{"type":"notebook-content"},"key":"qFh9Xa0WIQ"},{"type":"block","children":[{"type":"code","lang":"python","executable":true,"value":"# Generate points along the fitted line for plotting\nx_fit = np.linspace(min(x), max(x), 100)\ny_fit = slope * x_fit + intercept\n\n# Plot the data points and the fitted line\nplt.figure(figsize=(8, 6))\nplt.scatter(x, y, color=\"blue\", label=\"Data\")\nplt.plot(x_fit, y_fit, color=\"red\", label=\"Fitted Line\")\nplt.xlabel(\"x\", fontsize=14)\nplt.ylabel(\"y\", fontsize=14)\nplt.legend()\nplt.show()","key":"SdMJpSrrdh"},{"type":"output","id":"_fE_aLdG605DcuUmGq9sc","data":[{"output_type":"display_data","metadata":{},"data":{"image/png":{"content_type":"image/png","hash":"df23e494163385452b38e8f50d5e57f1","path":"/build/df23e494163385452b38e8f50d5e57f1.png"},"text/plain":{"content":"<Figure size 800x600 with 1 Axes>","content_type":"text/plain"}}}],"key":"GAxnllad0k"}],"data":{"type":"notebook-code"},"key":"qTHpVbiKj5"},{"type":"block","children":[{"type":"heading","depth":2,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"A practical astronomy application: The M-sigma Relation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"kUJD8U08Js"}],"identifier":"a-practical-astronomy-application-the-m-sigma-relation","label":"A practical astronomy application: The M-sigma Relation","html_id":"a-practical-astronomy-application-the-m-sigma-relation","implicit":true,"key":"ZhfFk3iVuL"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"The M-sigma relation, also known as the black hole mass-velocity dispersion relation, is an empirical correlation observed between the mass of supermassive black holes (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"tqOhredBrL"},{"type":"inlineMath","value":"M_{bh}","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"html":"<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>M</mi><mrow><mi>b</mi><mi>h</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">M_{bh}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361em;\"><span style=\"top:-2.55em;margin-left:-0.109em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">bh</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>","key":"whdW7q8PUC"},{"type":"text","value":") at the centers of galaxies and the velocity dispersion (","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"CkAHf5TmqH"},{"type":"text","value":"σ","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"PikHWMTSVL"},{"type":"text","value":") of stars in the bulges of those galaxies. This relationship suggests that there is a tight connection between the properties of galaxies and the central black holes they host. Specifically, galaxies with larger velocity dispersions tend to harbor more massive black holes at their centers.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"u6vuPrMfLT"}],"key":"N9n5PhZQIo"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"The M-sigma relation has profound implications for our understanding of galaxy formation and evolution, as well as the co-evolution of galaxies and their central black holes. It provides valuable insights into the mechanisms governing the growth and regulation of supermassive black holes, the role of feedback processes in galaxy evolution, and the relationship between the properties of galactic bulges and their central black holes.","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"T6WyWdXBCD"}],"key":"pR7dnYMBDQ"}],"data":{"type":"notebook-content"},"key":"dN2X7DLSXb"},{"type":"block","children":[{"type":"admonition","class":"tip","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Exercise 1: Linear Regression for the M-sigma Relation","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"z5mHjsltqN"}],"key":"gKPPBxgf8i"},{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"We will apply the concept of linear regression to analyze the M-sigma relation using observational data. We have provided a dataset containing measurements of black hole masses (M_bh), velocity dispersions (sigma), and their associated uncertainties. Your task is to perform linear regression to fit both the black hole mass as a function of velocity dispersion (M_bh vs. sigma) and velocity dispersion as a function of black hole mass (sigma vs. M_bh).","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"bnuTOFFEAa"}],"key":"nQbS4xGim2"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":6,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":6,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"strong","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"Load the Data","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"r2wwJgjcKr"}],"key":"M1VppEav4u"},{"type":"text","value":": Read the ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"y8Na7Vkop8"},{"type":"inlineCode","value":"m-sigma","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"fzzno6MrIB"},{"type":"text","value":" dataset sent in Slack into a Pandas dataframe and inspect the first few rows to familiarize yourself with the structure of the data.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"OwbDLG4Ong"}],"key":"hNTjNf7umm"}],"key":"vvRsgY7bcp"},{"type":"listItem","spread":true,"position":{"start":{"line":8,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"strong","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"children":[{"type":"text","value":"Plot the Data","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"cb7aBuK6uq"}],"key":"qWK6n3jGjH"},{"type":"text","value":": Create a scatter plot of the log of the black hole mass (log(M_bh)) against velocity dispersion (sigma), with error bars representing the uncertainties in both quantities.","position":{"start":{"line":8,"column":1},"end":{"line":8,"column":1}},"key":"pFfBVtwFpB"}],"key":"DKMTmQIfs5"}],"key":"ZQGJ9PuXL3"},{"type":"listItem","spread":true,"position":{"start":{"line":10,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"strong","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"Perform Linear Regression","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"swK5JycWAO"}],"key":"vZfhUmYxov"},{"type":"text","value":":","position":{"start":{"line":10,"column":1},"end":{"line":10,"column":1}},"key":"r4YuB63lZ0"}],"key":"TUw8pm5YKs"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Fit a linear regression model to the data, treating M_bh as the dependent variable (y) and sigma as the independent variable (x) and incorporate the errors on sigma in the fit","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"Nnrg3Rt0Z0"}],"key":"HYjJnF468Z"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":17,"column":1}},"children":[{"type":"text","value":"Fit another linear regression model, treating sigma as the dependent variable (y) and M_bh as the independent variable (x) and incorporate the errors on M_bh in the fit.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"EdZvd1dqOC"},{"type":"admonition","kind":"hint","class":"dropdown","children":[{"type":"admonitionTitle","children":[{"type":"text","value":"Hint: Incorporating Errors","position":{"start":{"line":13,"column":1},"end":{"line":13,"column":1}},"key":"tEp20NKrQH"}],"key":"AcRAcziZWW"},{"type":"paragraph","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"children":[{"type":"text","value":"Look into the weights keyword argument for ","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"ZS6QCRKDHl"},{"type":"inlineCode","value":"np.polyfit","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"i0aWU1v7bM"},{"type":"text","value":" to figure out how to incorporate errors.","position":{"start":{"line":15,"column":1},"end":{"line":15,"column":1}},"key":"X1ZDFaiYOL"}],"key":"iFgt2YuDrH"}],"key":"xvTDVbc6mv"}],"key":"dEvF4Gb5pc"}],"key":"wpCDGCyp6W"}],"key":"LsaZsMcQSp"},{"type":"listItem","spread":true,"position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"strong","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"Visualize the Results","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"imhRVGzufW"}],"key":"uN3nXCUVlF"},{"type":"text","value":": Plot the regression lines along with the data points to observe how well they fit the M-sigma relation. Compare the slopes and intercepts of the regression lines for both fits.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"ZVhbtoY5Th"}],"key":"cgLlWydHDj"}],"key":"mJ7coBOApU"}],"key":"of9a1VJEWd"}],"key":"CvRGPnr8fD"}],"data":{"type":"notebook-content"},"key":"YpaCdOOESv"},{"type":"block","children":[{"type":"heading","depth":1,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"So, how can we incapsulate uncertainty in the fitted parameters?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"FxJvPlhcCu"}],"identifier":"so-how-can-we-incapsulate-uncertainty-in-the-fitted-parameters","label":"So, how can we incapsulate uncertainty in the fitted parameters?","html_id":"so-how-can-we-incapsulate-uncertainty-in-the-fitted-parameters","implicit":true,"key":"aQKscFR1w9"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"As we can see, fitting the dispersion as a function of black hole mass, weighting by the uncertainties in velocity dispersion (and ignoring uncertainties in M_bh) produced a different fit! It should be clear then that we are not done — and that neither of these fits are likely well characterized in terms of the uncertainties in the data.","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"QIv4YL3nMN"}],"key":"c5KZb46VhJ"},{"type":"paragraph","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"There are many ways to deal with this problem, including higher-complexity fits that include uncertainties in both x and y. One different, but conceptually simpler way we can estimate our uncertainty in the parameters of the fit (slope and intercept) is via ","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"ujjHNSiwaz"},{"type":"strong","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"children":[{"type":"text","value":"bootstrap resampling or perturbative re-fitting","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"sSAMalBNbZ"}],"key":"d4FvKviCU5"},{"type":"text","value":".","position":{"start":{"line":5,"column":1},"end":{"line":5,"column":1}},"key":"gEpbyNi06L"}],"key":"RSKYGEcqXg"},{"type":"heading","depth":2,"position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"children":[{"type":"text","value":"Bootstrap Resampling","position":{"start":{"line":7,"column":1},"end":{"line":7,"column":1}},"key":"WHxh4u3LSb"}],"identifier":"bootstrap-resampling","label":"Bootstrap Resampling","html_id":"bootstrap-resampling","implicit":true,"key":"uzap3p7BWv"},{"type":"paragraph","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"children":[{"type":"text","value":"Bootstrapping is a resampling technique used in statistics to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the observed data. It is particularly useful when the underlying population distribution is unknown or difficult to model.","position":{"start":{"line":9,"column":1},"end":{"line":9,"column":1}},"key":"yGNolSQJxi"}],"key":"jqNSU8rqd0"},{"type":"list","ordered":true,"start":1,"spread":false,"position":{"start":{"line":11,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"children":[{"type":"text","value":"Sample Creation: Bootstrapping starts with the creation of multiple bootstrap samples by randomly selecting observations from the original dataset with replacement. This means that each observation in the original dataset has the same chance of being selected for each bootstrap sample, and some observations may be selected multiple times while others may not be selected at all.","position":{"start":{"line":11,"column":1},"end":{"line":11,"column":1}},"key":"j8mii5Xmor"}],"key":"kZQni3emt3"},{"type":"listItem","spread":true,"position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"Statistical Estimation: After creating the bootstrap samples, the statistic of interest (e.g., mean, median, standard deviation, regression coefficient) is calculated for each bootstrap sample. This results in a distribution of the statistic across the bootstrap samples, known as the bootstrap distribution. In our case, this would be fitting our line to the data and seeing how much the paramters (slope and intercept) change for each fit. Now, we have some way of expressing uncertainty in our fitted parameters!\n","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"AR8MBnbeqB"},{"type":"image","url":"/build/bootstrap_resampling-19edf9abbf7bf206cfe7896795c20131.png","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"FAOOWGPdqJ","urlSource":"bootstrap_resampling.png"},{"type":"text","value":"\nOn Wednesday, we will start with an exercise using bootstrapping to estimate the uncertainty on our fit for the M-sigma relation!","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"Glm4SQSb4g"}],"key":"uJFxASBGzo"}],"key":"wkahjazUs2"}],"data":{"type":"notebook-content"},"key":"KOyLrYjYYo"},{"type":"block","children":[{"type":"heading","depth":1,"position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"children":[{"type":"text","value":"What about fitting arbitrary functions?","position":{"start":{"line":1,"column":1},"end":{"line":1,"column":1}},"key":"J1NJLiC2rZ"}],"identifier":"what-about-fitting-arbitrary-functions","label":"What about fitting arbitrary functions?","html_id":"what-about-fitting-arbitrary-functions","implicit":true,"key":"ghC1CgXcQs"},{"type":"paragraph","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"children":[{"type":"text","value":"Everything we’ve done so far pertains to fitting polynomials to data, but what if you want to fit a differnt type of function (like a Gaussian distribution)? We will go into this on Wednesday!","position":{"start":{"line":3,"column":1},"end":{"line":3,"column":1}},"key":"Kexn6jF2Cr"}],"key":"y8FOLcuDeC"}],"data":{"type":"notebook-content"},"key":"ePQ2wN9SM4"}],"key":"htsywpuAA3"},"references":{"cite":{"order":[],"data":{}}},"footer":{"navigation":{"prev":{"title":"Pandas","url":"/pandas","group":"Week  5"},"next":{"title":"Unix Scavenger Hunt!","url":"/hunt","group":"Week  1"}}},"domain":"http://localhost:3001"}